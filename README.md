# C474 Project 2
Adaptive Multi-Agent Chatbot System using Ollama.

### Team Members
Bhavya Bugude - 40270772
<br> Inas Fawzi - 40208675
<br> Sofia Valiante - 40191897
<br> Sana Antoun - 40209806
<br> Omar Alshanyour - 40209637 

### Files Breakdown
```
multi_agent_chatbot/
├── agents/
│   ├── general_agent.py
│   ├── admissions_agent.py
│   ├── ai_agent.py
│   └── router.py
├── memory/
│   └── memory_manager.py
├── api/
│   └── main.py
├── utils/
│   └── knowledge_integration.py
├── config/
│   └── settings.py
├── docs/
│   ├── technical_report.txt
│   ├── execution_guide.txt
│   ├── requirements.txt
│   ├── demo_video.mov
│   └── performance_evaluation.txt
└── README.md
```

### How to get up and running

_(this may be changed as the project progresses)_ 
<br> First, you must install the packages and libraries. The project dependencies can be found in `.\docs\requirements.tsx` of the project directory, and are also listed below:
- fastapi
- uvicorn
- langchain
- wikipedia
- ollama
- pydantic
- langchain_ollama
- langchain-community

Run the following command to install the necessary dependencies:
    `pip install fastapi uvicorn langchain ollama wikipedia pydantic langchain_ollama langchain-community`
or run the command in the terminal
    `pip install -r .\docs\requirements.tsx`. 
If the IDE still cannot recognize the libraries and packages, install them independently, using the IDE suggestions or refresh your Python Interpreter.


Steps to run 
<br> _(this may be changed as the project progresses)_:

1. Pull the LLMs you will be using from Ollama
    <br> (In a terminal outside of your IDE)
    - Run the command `ollama` to confirm it is installed. Alternatively, you may run Ollama
      by clicking its application icon in your local environment.
    - Run the command `ollama pull mistral` to download the model `mistral` from the internet. It may take some time and make sure you have the space on your computer for it. `Mistral` is a smaller LLM and will be used in the project. 
    - Confirm that the model is downloaded with the command `ollama list` and view the model.

2. Run the FastAPI (which you will be querying the chatbots, and therefore, the LLms)
    <br> (In your IDE terminal)
    - run `uvicorn api.main:app --reload` (where uvicorn is the command to run the file found at api.main of the project directory).
    - open a browser and enter the link `http://127.0.0.1:8000/docs` to access SwaggerUI.
    - _(Alternatively)_ run `.\api\main.py` in your IDE which will open a connection with SwaggerUI.

3. Now that FastAPI is running, we can use our chatbots
    - Navigate to the `POST /chat endpoint` on the webpage.
    - Click the button "Try it out".
    - Modify the request body _'message'_ with your prompt or query for the agents. The request body will look like this (with your query):
        ```
        {
            "message": "What is the weather in New York today?"
        }
        ```
    - Click "Execute" and wait for the relevant agent to generate its response. This may take some time. 
    - Once the page has finished loading, scroll down to the "Response" section of the page and view the "Server Response".
      The response generated by the agent can be view in the block "Response body" in json format,
      along with other details including the curl, request URL and response headers. An example response is below for the query posed,
        ```
        {
            "response": "To provide you with the most accurate information, I would typically use a weather API to get the current weather conditions for a specific location. Since I don't have access to such an API at this moment, let me share with you that as of now, I cannot give you the exact weather in New York today. However, you can easily find out by checking a reliable weather website or app. Have a great day!"
        }
        ```

4. When your done, close everything
    - Stop running the FastAPI in your IDE terminal.
    - Stop running Ollama.

### Architecture Overview

**Agents**
<br> _General Agent_: Handles generic queries.
<br> _Admissions Agent_: Specializes in answering questions about Concordia University's Computer Science admissions.
<br> _AI Agent_: Focuses on AI-related questions and topics.

**Core Components**
<br> _Router_: Detects user intent and delegates to the appropriate specialized agent.
<br> _Memory Management_: LangChain’s memory modules for maintaining session context.
<br> _External Knowledge_: Wikipedia API or other APIs for up-to-date info.

**Tech Stack**
<br> _LLM Backend_: Ollama (e.g., running LLaMA, Mistral, etc.)
<br> _LangChain_: Agent routing, memory, prompt templates.
<br> _FastAPI_: RESTful API interface.
<br> _Python_: Core logic and orchestration.
